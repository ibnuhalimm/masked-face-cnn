{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.exists('datasets'):\n",
    "    shutil.rmtree('datasets')\n",
    "    \n",
    "if os.path.isfile('masked-face-datasets.zip'):\n",
    "    os.remove('masked-face-datasets.zip')\n",
    "    \n",
    "!wget https://jarkom-ibm-cloud-2022.s3.jp-tok.cloud-object-storage.appdomain.cloud/masked-face-datasets.zip\n",
    "!unzip masked-face-datasets.zip\n",
    "\n",
    "os.rename('masked-face-datasets', 'datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from load_image import *\n",
    "from base_model import *\n",
    "from plot import *\n",
    "\n",
    "model = model_VGG16()\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=Nadam(\n",
    "                learning_rate=0.001,\n",
    "              ),\n",
    "              metrics=[\"accuracy\"])\n",
    "model_summary = model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "checkpoint = ModelCheckpoint('best_model.h5',  save_best_only=True)\n",
    "\n",
    "test_time = time.strftime('%Y%m%d_%H%M')\n",
    "start_history = time.perf_counter()\n",
    "\n",
    "epochs = 30\n",
    "steps_size_train = train_data_augmented.n // train_data_augmented.batch_size\n",
    "steps_size_validation = validation_data.n // validation_data.batch_size\n",
    "\n",
    "history = model.fit(train_data_augmented,\n",
    "                    steps_per_epoch=steps_size_train,\n",
    "                    validation_data=validation_data,\n",
    "                    validation_steps=steps_size_validation,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=[\n",
    "                        checkpoint,\n",
    "                        # earlystopping\n",
    "                    ]\n",
    "                    )\n",
    "\n",
    "elapsed_history = time.perf_counter() - start_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_scores = model.evaluate(\n",
    "    train_data_augmented,\n",
    "    batch_size=validation_data.batch_size\n",
    "    )\n",
    "\n",
    "train_loss = train_eval_scores[0]\n",
    "train_acc = round(train_eval_scores[1] * 100, 2)\n",
    "print(f'Training Loss : {train_loss}')\n",
    "print(f'Training Accuracy : {train_acc}%\\n')\n",
    "\n",
    "val_eval_scores = model.evaluate(\n",
    "    validation_data,\n",
    "    batch_size=validation_data.batch_size\n",
    "    )\n",
    "\n",
    "val_loss = val_eval_scores[0]\n",
    "val_acc = round(val_eval_scores[1] * 100, 2)\n",
    "print(f'Validation Loss : {val_loss}')\n",
    "print(f'Validation Accuracy : {val_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## Create directory to save the result\n",
    "result_dir = f'results/{test_case}/{test_time}'\n",
    "data_test_dir = f'{result_dir}/data_test'\n",
    "\n",
    "if (os.path.exists(result_dir) == False):\n",
    "    os.makedirs(result_dir)\n",
    "        \n",
    "if not os.path.exists(data_test_dir):\n",
    "    os.makedirs(data_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "## move best_model file\n",
    "if os.path.isfile('best_model.h5'):\n",
    "    shutil.move('best_model.h5', f'{result_dir}')\n",
    "\n",
    "## Write History Time and Evaluate into a txt file\n",
    "f = open(f'{result_dir}/result_data.txt', 'a')\n",
    "\n",
    "f.write('- fit time \\t: %.3f seconds' % elapsed_history)\n",
    "f.write('\\n\\n')\n",
    "\n",
    "f.write(f'- image_width \\t: {img_width}\\n')\n",
    "f.write(f'- image_height \\t: {img_height}\\n')\n",
    "f.write('\\n\\n')\n",
    "\n",
    "f.write(f'- batch size dataset train: \\t{train_data_augmented.batch_size}\\n')\n",
    "f.write(f'- batch size dataset val: \\t{validation_data.batch_size}\\n')\n",
    "f.write('\\n\\n')\n",
    "\n",
    "f.write(f'Fit model\\n')\n",
    "f.write(f'- epoch \\t: {epochs}\\n')\n",
    "f.write('\\n\\n')\n",
    "\n",
    "f.write(f'- train_loss \\t: {train_loss}\\n')\n",
    "f.write(f'- train_acc \\t: {train_acc}%\\n')\n",
    "f.write('\\n\\n')\n",
    "\n",
    "f.write(f'- val_loss \\t: {val_loss}\\n')\n",
    "f.write(f'- val_acc \\t: {val_acc}%\\n')\n",
    "f.close()\n",
    "\n",
    "# Save the model\n",
    "model_filename = \"model\"\n",
    "model_json = model.to_json()\n",
    "with open(f'{result_dir}/{model_filename}.json', \"w\") as json_file:\n",
    "    json_file.write(f'{result_dir}/{model_json}')\n",
    "\n",
    "model.save(f'{result_dir}/{model_filename}.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = plot_loss_curves(history)\n",
    "loss_plot.savefig(f'{result_dir}/loss_graph.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_plot = plot_accuracy_curves(history)\n",
    "acc_plot.savefig(f'{result_dir}/acc_graph.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image as krimg\n",
    "import matplotlib.image as mpimpg\n",
    "import glob\n",
    "import os\n",
    "import ntpath\n",
    "    \n",
    "faceimagedir = 'datasets/'+ test_case +'/test'\n",
    "best_model = load_model(f'{result_dir}/best_model.h5')\n",
    "\n",
    "images_paths = []\n",
    "for img_path in glob.glob(faceimagedir + '/*.jpg'):\n",
    "    images_paths.append(img_path)\n",
    "    \n",
    "for i, image in enumerate(images_paths):\n",
    "    img = krimg.load_img(image, target_size=(img_width, img_height))\n",
    "    image_filename = ntpath.basename(image)\n",
    "\n",
    "    image_to_predict = np.expand_dims(img, axis=0)\n",
    "    predictions = best_model.predict(image_to_predict)\n",
    "    classes = np.argmax(predictions, axis = 1)\n",
    "    prediction_name = class_names[classes][0]\n",
    "    \n",
    "    if prediction_name:\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel(prediction_name)\n",
    "        plt.imshow(img)\n",
    "        plt.savefig(f'{result_dir}/data_test/{image_filename}', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_data_augmented.next()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# generate batch of images\n",
    "for i in range(0, 5):\n",
    "    plt.subplot(1, 5, 1 + i)\n",
    "    plt.imshow(img[i])\n",
    "\n",
    "plt.savefig(f'{result_dir}/augmentasi_data.jpg', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
